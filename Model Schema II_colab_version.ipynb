{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Model Schema II-checkpoint.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atorossian/FastCycle-GAN/blob/main/Model%20Schema%20II_colab_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45aKpIrqao7z"
      },
      "source": [
        "## Model Schema II"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7AWcLhGay5p",
        "outputId": "527be2a3-5b12-4cda-ebd1-0963a6bb5e89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-nyh3dw4_\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-nyh3dw4_\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=9d6603784df1a463762ac751c549b407f116d9e0e3c5adb2faec07e844759f82\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cseboako/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oILgY4zbao7-"
      },
      "source": [
        "# Importing libraries\n",
        "# Math and file management\n",
        "import numpy as np\n",
        "import re\n",
        "from tqdm.auto import tqdm\n",
        "import sys\n",
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import pickle\n",
        "import fnmatch\n",
        "import random\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "# For Model Building\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation,BatchNormalization, UpSampling2D, Dropout, Flatten, Dense, Input, LeakyReLU, Conv2DTranspose,AveragePooling2D, Concatenate\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "# For image processing\n",
        "import skimage.color as imcolor\n",
        "import PIL as Image\n",
        "import matplotlib.pyplot as plt\n",
        "# For processing time measurement\n",
        "import time"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpxi0_Mnd9Y9",
        "outputId": "bcac3ac9-7698-429a-cd1e-639708413a90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "drive.mount('/content/gdrive/MyDrive/MPII Human Pose')\n",
        "!ls ./content/gdrive/MyDrive/MPII_Human_Pose"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-20ac53e745c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/MPII_Human_Pose'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls ./content/gdrive/MyDrive/MPII_Human_Pose'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mnormed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must be in a directory that exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m   \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_signal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGKILL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must be in a directory that exists"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "q35ria2Lao8B"
      },
      "source": [
        "# **THIS STEP SHOULD BE MADE AD HOC DEPENDING ON THE SOURCE OF THE DATASET**\n",
        "# IN THIS CASE WE HAVE TO CHANGE THE DIRECTORY TO THE PATH WHERE THE DATASET IS LOCATED\n",
        "def extract(pattern, compression_format, path):\n",
        "    cwd=os.chdir(path)\n",
        "    os.walk(cwd)\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for name in files:\n",
        "            if fnmatch.fnmatch(name, pattern):\n",
        "                tar = tarfile.open(name, compression_format)\n",
        "                tar.extractall()\n",
        "                tar.close()\n",
        "    return\n",
        "\n",
        "def download(url):\n",
        "    '''\n",
        "    Downloads the file of a given url\n",
        "    '''\n",
        "    filename = url.split(\"/\")[-1]\n",
        "    if not os.path.exists(filename):\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "        \n",
        "def upload_to_s3(channel, file):\n",
        "    '''\n",
        "    Save file in a given folder in the S3 bucket\n",
        "    '''\n",
        "    uploaded = files.upload()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "IURwXYq9ao8C",
        "outputId": "b7c68561-4f6d-4739-d05f-d0217d2b456e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "# MPII Human Pose\n",
        "#download('https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz')\n",
        "upload_to_s3('people', 'mpii_human_pose_v1.tar.gz')\n",
        "\n",
        "#untar the file\n",
        "!tar xvzf mpii_human_pose_v1.tar.gz\n",
        "\n",
        "\n",
        "#extract('*.tar.gz',\n",
        "#        'r:gz',\n",
        "#        r'\\Académico\\Posgrados\\2019 - Maestría en Ciencia de Datos e Innovación Empresarial\\Tesis\\Datasets')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7363987894d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m extract('*.tar.gz',\n\u001b[1;32m      2\u001b[0m         \u001b[0;34m'r:gz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         r'\\Académico\\Posgrados\\2019 - Maestría en Ciencia de Datos e Innovación Empresarial\\Tesis\\Datasets')\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-2a9df7fa3006>\u001b[0m in \u001b[0;36mextract\u001b[0;34m(pattern, compression_format, path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# IN THIS CASE WE HAVE TO CHANGE THE DIRECTORY TO THE PATH WHERE THE DATASET IS LOCATED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\Académico\\\\Posgrados\\\\2019 - Maestría en Ciencia de Datos e Innovación Empresarial\\\\Tesis\\\\Datasets'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntsf6Zj7ao8D"
      },
      "source": [
        "def image_read(file, size=(256,256)):\n",
        "    '''\n",
        "    This function loads and resizes the image to the passed size and transforms that image into an array\n",
        "    Default image size is set to be 256x256\n",
        "    '''\n",
        "    img = image.load_img(file, target_size=size)\n",
        "    img = image.img_to_array(img)\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T782B8sao8D"
      },
      "source": [
        "def image_convert(image_paths,size=256,channels=3):\n",
        "    '''\n",
        "    Redimensions images to Numpy arrays of a certain size and channels. Default values are set to 256x256x3 for coloured\n",
        "    images.\n",
        "    Parameters:\n",
        "    file_paths: a path to the image files\n",
        "    size: an int or a 2x2 tuple to define the size of an image\n",
        "    channels: number of channels to define in the numpy array\n",
        "    '''\n",
        "    # If size is an int\n",
        "    if isinstance(size, int):\n",
        "        # build a zeros matrix of the size of the image\n",
        "        all_images_to_array = np.zeros((len(image_paths), size, size, channels), dtype='int64')\n",
        "        for ind, i in enumerate(image_paths):\n",
        "            # reads image\n",
        "            img = image_read(i)\n",
        "            all_images_to_array[ind] = img.astype('int64')\n",
        "        print('All Images shape: {} size: {:,}'.format(all_images_to_array.shape, all_images_to_array.size))\n",
        "    else:\n",
        "        all_images_to_array = np.zeros((len(image_paths), size[0], size[1], channels), dtype='int64')\n",
        "        for ind, i in enumerate(image_paths):\n",
        "            img = read_img(i)\n",
        "            all_images_to_array[ind] = img.astype('int64')\n",
        "        print('All Images shape: {} size: {:,}'.format(all_images_to_array.shape, all_images_to_array.size))\n",
        "    return all_images_to_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwBzs1sbao8E"
      },
      "source": [
        "def find(pattern, path):\n",
        "    result = []\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for name in files:\n",
        "            if fnmatch.fnmatch(name, pattern):\n",
        "                result.append(os.path.join(root, name))\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "WgN_aHdBao8F"
      },
      "source": [
        "image_paths=find('*.jpg', r'\\Académico\\Posgrados\\2019 - Maestría en Ciencia de Datos e Innovación Empresarial\\Tesis\\Datasets\\images2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2n2kkalao8G",
        "outputId": "395bfae5-f7d0-426c-d63d-a4ae8304dc3a"
      },
      "source": [
        "X_train=image_convert(image_paths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All Images shape: (10, 256, 256, 3) size: 1,966,080\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23dGXok6ao8H"
      },
      "source": [
        "def rgb_to_lab(img, l=False, ab=False):\n",
        "    \"\"\"\n",
        "    Takes in RGB channels in range 0-255 and outputs L or AB channels in range -1 to 1\n",
        "    \"\"\"\n",
        "    img = img / 255\n",
        "    lum = imcolor.rgb2lab(img)[:,:,0]\n",
        "    lum = (lum / 50) - 1\n",
        "    lum = lum[...,np.newaxis]\n",
        "\n",
        "    a_b = imcolor.rgb2lab(img)[:,:,1:]\n",
        "    a_b = (a_b + 128) / 255 * 2 - 1\n",
        "    if l:\n",
        "        return lum\n",
        "    else: return a_b\n",
        "\n",
        "def lab_to_rgb(img):\n",
        "    \"\"\"\n",
        "    Takes in LAB channels in range -1 to 1 and out puts RGB chanels in range 0-255\n",
        "    \"\"\"\n",
        "    new_img = np.zeros((256,256,3))\n",
        "    for i in range(len(img)):\n",
        "        for j in range(len(img[i])):\n",
        "            pix = img[i,j]\n",
        "            new_img[i,j] = [(pix[0] + 1) * 50,(pix[1] +1) / 2 * 255 - 128,(pix[2] +1) / 2 * 255 - 128]\n",
        "    new_img = imcolor.lab2rgb(new_img) * 255\n",
        "    new_img = new_img.astype('uint8')\n",
        "    return new_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAX55Tonao8I"
      },
      "source": [
        "L = np.array([rgb_to_lab(image, l=True) for image in X_train])\n",
        "AB = np.array([rgb_to_lab(image, ab=True) for image in X_train])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDUhR8AQao8J"
      },
      "source": [
        "L_AB_channels = (L,AB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQkOYlzYao8K"
      },
      "source": [
        "with open('l_ab_channels.p','wb') as f:\n",
        "        pickle.dump(L_AB_channels,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq_3j2C9ao8L"
      },
      "source": [
        "def load_images(filepath):\n",
        "    '''\n",
        "    Loads in pickle files, specifically the L and AB channels\n",
        "    '''\n",
        "    with open(filepath, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRr_8Qqrao8M"
      },
      "source": [
        "X_train_L, X_train_AB = load_images('l_ab_channels.p')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXi_t8reao8M"
      },
      "source": [
        "def resnet_block(x ,num_conv=2, num_filters=512,kernel_size=(3,3),padding='same',strides=2):\n",
        "    '''\n",
        "    This function defines a ResNet Block composed of two convolution layers and that returns the sum of the inputs and the\n",
        "    convolution outputs.\n",
        "    Parameters\n",
        "    x: is the tensor which will be used as input to the convolution layer\n",
        "    num_conv: is the number of convolutions inside the block\n",
        "    num_filters: is an int that describes the number of output filters in the convolution\n",
        "    kernel size: is an int or tuple that describes the size of the convolution window\n",
        "    padding: padding with zeros the image so that the kernel fits the input image or not. Options: 'valid' or 'same'\n",
        "    strides: is the number of pixels shifts over the input matrix. \n",
        "    '''\n",
        "    input=x\n",
        "    for i in range(num_conv):\n",
        "        \n",
        "        input=Conv2D(num_filters,kernel_size=kernel_size,padding=padding,strides=strides)(input)\n",
        "        input=InstanceNormalization()(input)\n",
        "        input=LeakyReLU(0.2)(input)\n",
        "\n",
        "\n",
        "    return (input + x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3Y0mnwZao8N"
      },
      "source": [
        "def generator(filters=64,num_enc_layers=4,num_resblock=4,name=\"Generator\"):\n",
        "    ''' \n",
        "    The generator per se is an autoencoder built by a series of convolution layers that initially extract features of the\n",
        "    input image.\n",
        "    '''\n",
        "\n",
        "    # defining input\n",
        "    x_0=Input(shape=(256,256,1))\n",
        "    \n",
        "    '''\n",
        "    Adding first layer of the encoder model: 64 filters, 5x5 kernel size, 2 so the input size is reduced to half,\n",
        "    input size is the image size: (256,256,1), number of channels 1 for the luminosity channel.\n",
        "    We will use InstanceNormalization through the model and Leaky Relu with and alfa of 0.2\n",
        "    as activation function for the encoder, while relu as activation for the decoder.\n",
        "    between both of them, in the latent space we insert 4 resnet blocks.\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    for lay in range(num_enc_layers):\n",
        "        x=Conv2D(filters*lay,(3,3),padding='same',strides=2,input_shape=(256,256,1))(x_0)\n",
        "        x=InstanceNormalization()(x)\n",
        "        x=LeakyReLU(0.2)(x)\n",
        "    \n",
        "    '''\n",
        "----------------------------------LATENT SPACE---------------------------------------------\n",
        "    '''\n",
        "    #for r in range(num_resblock):\n",
        "    #    x=resnet_block(x)    \n",
        "    '''\n",
        "----------------------------------LATENT SPACE---------------------------------------------\n",
        "    '''\n",
        "    \n",
        "    x=Conv2DTranspose(256,(3,3),padding='same',strides=2)(x)\n",
        "    x=InstanceNormalization()(x)\n",
        "    x=Activation('relu')(x)\n",
        "              \n",
        "    x=Conv2DTranspose(128,(3,3),padding='same',strides=2)(x)\n",
        "    x=InstanceNormalization()(x)\n",
        "    x=Activation('relu')(x)\n",
        "    \n",
        "    x=Conv2DTranspose(64,(3,3),padding='same',strides=2)(x)\n",
        "    x=InstanceNormalization()(x)\n",
        "    x=Activation('relu')(x)\n",
        "              \n",
        "    x=Conv2DTranspose(32,(5,5),padding='same',strides=2)(x)\n",
        "    x=InstanceNormalization()(x)\n",
        "    x=Activation('relu')(x)\n",
        "    \n",
        "    x=Conv2D(2,(3,3),padding='same')(x)\n",
        "    output=Activation('tanh')(x)\n",
        "    \n",
        "    model=Model(x_0,output,name=name)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwVnD4dRao8N"
      },
      "source": [
        "def discriminator(name=\"Discriminator\"):\n",
        "    \n",
        "    # defining input\n",
        "    x_0=Input(shape=(256,256,2))\n",
        "    \n",
        "    x=Conv2D(32,(3,3), padding='same',strides=2,input_shape=(256,256,2))(x_0)\n",
        "    x=LeakyReLU(0.2)(x)\n",
        "    x=Dropout(0.25)(x)\n",
        "        \n",
        "    x=Conv2D(64,(3,3),padding='same',strides=2)(x)\n",
        "    x=BatchNormalization()(x)\n",
        "    x=LeakyReLU(0.2)(x)\n",
        "    x=Dropout(0.25)(x)\n",
        "        \n",
        "        \n",
        "    x=Conv2D(128,(3,3), padding='same', strides=2)(x)\n",
        "    x=BatchNormalization()(x)\n",
        "    x=LeakyReLU(0.2)(x)\n",
        "    x=Dropout(0.25)(x)\n",
        "        \n",
        "        \n",
        "    x=Conv2D(256,(3,3), padding='same',strides=2)(x)\n",
        "    x=BatchNormalization()(x)\n",
        "    x=LeakyReLU(0.2)(x)\n",
        "    x=Dropout(0.25)(x)\n",
        "        \n",
        "        \n",
        "    x=Flatten()(x)\n",
        "    x=Dense(1)(x)\n",
        "    output=Activation('sigmoid')(x)\n",
        "        \n",
        "    model=Model(x_0,output,name=name)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFoo-P1wao8O"
      },
      "source": [
        "d_image_shape = (256,256,2)\n",
        "g_image_shape = (256,256,1)\n",
        "discriminator = discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', \n",
        "                      optimizer=Adam(lr=0.00008,beta_1=0.5,beta_2=0.999), \n",
        "                    metrics=['accuracy']) \n",
        "  \n",
        "#Making the Discriminator untrainable so that the generator can learn from fixed gradient \n",
        "discriminator.trainable = False\n",
        "\n",
        "# Build the Generator \n",
        "generator = generator()\n",
        "  \n",
        "#Defining the combined model of the Generator and the Discriminator \n",
        "l_channel = Input(shape=g_image_shape)\n",
        "image = generator(l_channel) \n",
        "valid = discriminator(image)\n",
        "  \n",
        "combined_network = Model(l_channel, valid) \n",
        "combined_network.compile(loss='binary_crossentropy', \n",
        "                         optimizer=Adam(lr=0.0001,beta_1=0.5,beta_2=0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJg-c6nPao8Q"
      },
      "source": [
        "#creates lists to log the losses and accuracy\n",
        "gen_losses = []\n",
        "disc_real_losses = []\n",
        "disc_fake_losses=[] \n",
        "disc_acc = []\n",
        "\n",
        "#train the generator on a full set of 320 and the discriminator on a half set of 160 for each epoch\n",
        "#discriminator is given real and fake y's while generator is always given real y's\n",
        "n = 320\n",
        "y_train_fake = np.zeros([160,1])\n",
        "y_train_real = np.ones([160,1])\n",
        "y_gen = np.ones([n,1])\n",
        "\n",
        "#Optional label smoothing\n",
        "#y_train_real -= .1\n",
        "\n",
        "\n",
        "#Pick batch size and number of epochs, number of epochs depends on the number of photos per epoch set above\n",
        "num_epochs=10\n",
        "batch_size=32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UInLKWo-ao8R",
        "outputId": "237cd97b-06eb-41e5-c056-f1c14f2d7a13",
        "colab": {
          "referenced_widgets": [
            "41970a580cea4bb1a6a2fa911bfcfc1b"
          ]
        }
      },
      "source": [
        "for epoch in tqdm(range(1,num_epochs+1)):\n",
        "    #shuffle L and AB channels then take a subset corresponding to each networks training size\n",
        "    np.random.shuffle(X_train_L)\n",
        "    l = X_train_L[:n]\n",
        "    np.random.shuffle(X_train_AB)\n",
        "    ab = X_train_AB[:160]\n",
        "    \n",
        "    fake_images = generator.predict(l[:160], verbose=1)\n",
        "    \n",
        "    #Train on Real AB channels\n",
        "    d_loss_real = discriminator.fit(x=ab, y= y_train_real,batch_size=32,epochs=1,verbose=1) \n",
        "    disc_real_losses.append(d_loss_real.history['loss'][-1])\n",
        "    \n",
        "    #Train on fake AB channels\n",
        "    d_loss_fake = discriminator.fit(x=fake_images,y=y_train_fake,batch_size=32,epochs=1,verbose=1)\n",
        "    disc_fake_losses.append(d_loss_fake.history['loss'][-1])\n",
        "    \n",
        "    #append the loss and accuracy and print loss\n",
        "    disc_acc.append(d_loss_fake.history['acc'][-1])\n",
        "    \n",
        "\n",
        "    #Train the gan by producing AB channels from L\n",
        "    g_loss = combined_network.fit(x=l, y=y_gen,batch_size=32,epochs=1,verbose=1)\n",
        "    #append and print generator loss\n",
        "    gen_losses.append(g_loss.history['loss'][-1])\n",
        "   \n",
        "    #every 50 epochs it prints a generated photo and every 100 it saves the model under that epoch\n",
        "    if epoch % 50 == 0:\n",
        "        print('Reached epoch:',epoch)\n",
        "        pred = generator.predict(X_test_L[2].reshape(1,256,256,1))\n",
        "        img = lab_to_rgb(np.dstack((X_test_L[2],pred.reshape(256,256,2))))\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "        if epoch % 100 == 0:\n",
        "              generator.save('generator_' + str(epoch)+ '_v3.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41970a580cea4bb1a6a2fa911bfcfc1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-162-e4d3d038d2a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_AB\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m160\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mfake_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m160\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#Train on Real AB channels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1462\u001b[1;33m                                             callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, op, message)\u001b[0m\n\u001b[0;32m    363\u001b[0m   \"\"\"\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m     \u001b[1;34m\"\"\"Creates a `ResourceExhaustedError`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     super(ResourceExhaustedError, self).__init__(node_def, op, message,\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MneiCa0Dao8S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}