{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Schema Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to create in a simple format the schema of the solution proposed to colorize pictures with a Cycle-GAN accelerated with FFT convolutions.<p>To create a simple model schema this notebook will present the code for a Cycle-GAN built as a MVP (Minimum Viable Product) that works with the problem proposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from skimage import color\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, BatchNormalization, UpSampling2D, Dropout, Flatten, Dense, Input, LeakyReLU, Conv2DTranspose,AveragePooling2D, Concatenate\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "import boto3\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "#import tqdm seperately and use jupyter notebooks %%capture\n",
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter your bucket name and use boto3 to identify your region if you don't know it\n",
    "bucket = None\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_execution_role' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-84e33daffc65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#add your bucket then creat the containers to download files and send to bucket\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrole\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_execution_role\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbucket\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;31m# customize to your bucket\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_execution_role' is not defined"
     ]
    }
   ],
   "source": [
    "#add your bucket then creat the containers to download files and send to bucket\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = None # customize to your bucket\n",
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/image-classification:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/image-classification:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/image-classification:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/image-classification:latest'}\n",
    "training_image = containers[boto3.Session().region_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Required parameter name not set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-298d23c47f48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# MPII Human Pose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mupload_to_s3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'people'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mpii_human_pose_v1.tar.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#untar the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-298d23c47f48>\u001b[0m in \u001b[0;36mupload_to_s3\u001b[1;34m(channel, file)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchannel\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0ms3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBucket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\boto3\\resources\\factory.py\u001b[0m in \u001b[0;36mcreate_resource\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m             return partial(resource_cls, *positional_args,\n\u001b[1;32m--> 474\u001b[1;33m                            client=self.meta.client)(*args, **kwargs)\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[0mcreate_resource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\boto3\\resources\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                 raise ValueError(\n\u001b[1;32m--> 119\u001b[1;33m                     'Required parameter {0} not set'.format(identifier))\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Required parameter name not set"
     ]
    }
   ],
   "source": [
    "def download(url):\n",
    "    '''\n",
    "    Downloads the file of a given url\n",
    "    '''\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "        \n",
    "def upload_to_s3(channel, file):\n",
    "    '''\n",
    "    Save file in a given folder in the S3 bucket\n",
    "    '''\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = channel + '/' + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "\n",
    "# MPII Human Pose\n",
    "download('https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz')\n",
    "upload_to_s3('people', 'mpii_human_pose_v1.tar.gz')\n",
    "\n",
    "#untar the file\n",
    "!tar xvzf mpii_human_pose_v1.tar.gz\n",
    "\n",
    "\n",
    "#MIT coastal \n",
    "download('http://cvcl.mit.edu/scenedatabase/coast.zip')\n",
    "upload_to_s3('coast', 'coast.zip')\n",
    "\n",
    "#unzip the file\n",
    "!unzip coast.zip -d ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(file, size = (256,256)):\n",
    "    '''\n",
    "    reads the images and transforms them to the desired size\n",
    "    '''\n",
    "    img = image.load_img(file, target_size=size)\n",
    "    img = image.img_to_array(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_img_size(file_paths):\n",
    "    '''\n",
    "    converts all images to 256x256x3\n",
    "    '''\n",
    "    all_images_to_array = np.zeros((len(file_paths), 256, 256, 3), dtype='int64')\n",
    "    for ind, i in enumerate(file_paths):\n",
    "        img = read_img(i)\n",
    "        all_images_to_array[ind] = img.astype('int64')\n",
    "    print('All Images shape: {} size: {:,}'.format(all_images_to_array.shape, all_images_to_array.size))\n",
    "    return all_images_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob('./images/*.jpg')\n",
    "X_train = convert_img_size(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_lab(img, l=False, ab=False):\n",
    "    \"\"\"\n",
    "    Takes in RGB channels in range 0-255 and outputs L or AB channels in range -1 to 1\n",
    "    \"\"\"\n",
    "    img = img / 255\n",
    "    l = color.rgb2lab(img)[:,:,0]\n",
    "    l = l / 50 - 1\n",
    "    l = l[...,np.newaxis]\n",
    "\n",
    "    ab = color.rgb2lab(img)[:,:,1:]\n",
    "    ab = (ab + 128) / 255 * 2 - 1\n",
    "    if l:\n",
    "        return l\n",
    "    else: return ab\n",
    "\n",
    "def lab_to_rgb(img):\n",
    "    \"\"\"\n",
    "    Takes in LAB channels in range -1 to 1 and out puts RGB chanels in range 0-255\n",
    "    \"\"\"\n",
    "    new_img = np.zeros((256,256,3))\n",
    "    for i in range(len(img)):\n",
    "        for j in range(len(img[i])):\n",
    "            pix = img[i,j]\n",
    "            new_img[i,j] = [(pix[0] + 1) * 50,(pix[1] +1) / 2 * 255 - 128,(pix[2] +1) / 2 * 255 - 128]\n",
    "    new_img = color.lab2rgb(new_img) * 255\n",
    "    new_img = new_img.astype('uint8')\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.array([rgb_to_lab(image,l=True)for image in X_train])\n",
    "AB = np.array([rgb_to_lab(image,ab=True)for image in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_AB_channels = (L,AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('l_ab_channels.p','wb') as f:\n",
    "        pickle.dump(L_AB_channels,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(x):\n",
    "    # importing libraries\n",
    "    from keras.layers import Conv2D, MaxPooling2D, Activation, BatchNormalization, UpSampling2D, Dropout, Flatten, Dense, Input, LeakyReLU, Conv2DTranspose,AveragePooling2D, Concatenate\n",
    "    from tensorflow_addons import InstanceNormalization\n",
    "    \n",
    "    x1=Conv2D(512,(3,3),padding='same',strides=2)(x)\n",
    "    x1=InstanceNormalization()(x1)\n",
    "    x1=LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x1=Conv2D(512,(3,3),padding='same',strides=2)(x)\n",
    "    x1=InstanceNormalization()(x1)\n",
    "    x1=LeakyReLU(0.2)(x)\n",
    "\n",
    "    return (x1 + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    ''' \n",
    "    The generator per se is an autoencoder built by a series of convolution layers that initially extract features of the\n",
    "    input image.    \n",
    "    '''\n",
    "    # importing libraries\n",
    "    from keras.layers import Conv2D, MaxPooling2D, Activation, BatchNormalization, UpSampling2D, Dropout, Flatten, Dense, Input, LeakyReLU, Conv2DTranspose,AveragePooling2D, Concatenate\n",
    "    from tensorflow_addons import InstanceNormalization\n",
    "    \n",
    "    # defining input\n",
    "    input=Input(shape=(256,256,1))\n",
    "    x=input\n",
    "    \n",
    "    '''\n",
    "    Adding first layer of the encoder model: 64 filters, 5x5 kernel size, 2 so the input size is reduced to half,\n",
    "    input size is the image size: (256,256,1), number of channels 1 for the luminosity channel.\n",
    "    We will use InstanceNormalization through the model and Leaky Relu with and alfa of 0.2\n",
    "    as activation function for the encoder, while \n",
    "    '''\n",
    "    x=Conv2D(64,(5,5),padding='same',strides=2,input_shape=(256,256,1))(x)\n",
    "    x=InstanceNormalization()(x)\n",
    "    x=LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x=Conv2D(128,(3,3),padding='same',strides=2)(x)\n",
    "    x=InstanceNormalization()(x)\n",
    "    x=LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x=Conv2D(256,(3,3),padding='same',strides=2)(x)\n",
    "    x=InstanceNormalization()(x)\n",
    "    x=LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x=Conv2D(512,(3,3),padding='same',strides=2)(x)\n",
    "    x=InstanceNormalization()(x)\n",
    "    x=LeakyReLU(0.2)(x)\n",
    "    \n",
    "    '''\n",
    "----------------------------------LATENT SPACE---------------------------------------------\n",
    "    '''\n",
    "    x=resnet_block(x)\n",
    "    x=resnet_block(x)\n",
    "    x=resnet_block(x)\n",
    "    \n",
    "    '''\n",
    "----------------------------------LATENT SPACE---------------------------------------------\n",
    "    '''\n",
    "    \n",
    "    x=Conv2DTranspose(256,(3,3),padding='same',strides=2)(x)\n",
    "    x=InstanceNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "              \n",
    "    x=Conv2DTranspose(128,(3,3),padding='same',strides=2)(x)\n",
    "    x=InstanceNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    \n",
    "    x=Conv2DTranspose(64,(3,3),padding='same',strides=2)(x)\n",
    "    x=InstanceNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "              \n",
    "    x=Conv2DTranspose(32,(5,5),padding='same',strides=2)(x)\n",
    "    x=InstanceNormalization()(x)\n",
    "    x=Activation('relu')(x)\n",
    "    \n",
    "    x=Conv2D(2,(3,3),padding='same')(x)\n",
    "    output=Activation('tanh')(x)\n",
    "    \n",
    "    model=Model(input,output,name=\"Generator\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    # importing libraries\n",
    "    from keras.layers import Conv2D, MaxPooling2D, Activation, BatchNormalization, UpSampling2D, Dropout, Flatten, Dense, Input, LeakyReLU, Conv2DTranspose,AveragePooling2D, Concatenate\n",
    "    from tensorflow_addons import InstanceNormalization\n",
    "    \n",
    "    # defining input\n",
    "    input=Input(shape=(256,256,2))\n",
    "    x=input\n",
    "    \n",
    "    x=Conv2D(32,(3,3), padding='same',strides=2,input_shape=(256,256,2))(x)\n",
    "    x=LeakyReLU(0.2)(x)\n",
    "    x=Dropout(0.25)(x)\n",
    "        \n",
    "    x=Conv2D(64,(3,3),padding='same',strides=2)(x)\n",
    "    x=BatchNormalization()\n",
    "    x=LeakyReLU(0.2)(x)\n",
    "    x=Dropout(0.25)(x)\n",
    "        \n",
    "        \n",
    "    x=Conv2D(128,(3,3), padding='same', strides=2)(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=LeakyReLU(0.2)(x)\n",
    "    x=Dropout(0.25)(x)\n",
    "        \n",
    "        \n",
    "    x=Conv2D(256,(3,3), padding='same',strides=2)(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=LeakyReLU(0.2)(x)\n",
    "    x=Dropout(0.25)(x)\n",
    "        \n",
    "        \n",
    "    x=Flatten()(x)\n",
    "    x=Dense(1)(x)\n",
    "    output=Activation('sigmoid')(x)\n",
    "        \n",
    "    model=Model(input,output,name=\"Discriminator\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.5.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 1.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'keras_tensor' from 'tensorflow.python.keras.engine' (C:\\Users\\aleja\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4ce3f34a6433>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Building discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m discriminator.compile(loss='binary_crossentropy', \n\u001b[0;32m      4\u001b[0m                       \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00008\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     metrics=['accuracy'])\n",
      "\u001b[1;32m<ipython-input-10-cb72227dcac6>\u001b[0m in \u001b[0;36mdiscriminator\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# importing libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUpSampling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2DTranspose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mAveragePooling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInstanceNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# defining input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Local project imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\activations\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\activations\\gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_addons\\utils\\types.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# TODO: Remove once https://github.com/tensorflow/tensorflow/issues/44613 is resolved\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'keras_tensor' from 'tensorflow.python.keras.engine' (C:\\Users\\aleja\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Building discriminator\n",
    "discriminator=discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', \n",
    "                      optimizer=Adam(lr=0.00008,beta_1=0.5,beta_2=0.999), \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Building generator\n",
    "generator = generator()\n",
    "\n",
    "# Defining Inputs\n",
    "l=Input(shape=(256,256,1))\n",
    "image=generator(l)\n",
    "valid=discriminator(image)\n",
    "\n",
    "gan=Model(l,valid)\n",
    "combined_network.compile(loss='binary_crossentropy', \n",
    "                         optimizer=Adam(lr=0.0001,beta_1=0.5,beta_2=0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates lists to log the losses and accuracy\n",
    "gen_losses = []\n",
    "disc_real_losses = []\n",
    "disc_fake_losses=[] \n",
    "disc_acc = []\n",
    "\n",
    "#train the generator on a full set of 320 and the discriminator on a half set of 160 for each epoch\n",
    "#discriminator is given real and fake y's while generator is always given real y's\n",
    "n = 320\n",
    "y_train_fake = np.zeros([160,1])\n",
    "y_train_real = np.ones([160,1])\n",
    "y_gen = np.ones([n,1])\n",
    "\n",
    "#Optional label smoothing\n",
    "#y_train_real -= .1\n",
    "\n",
    "\n",
    "#Pick batch size and number of epochs, number of epochs depends on the number of photos per epoch set above\n",
    "num_epochs=1500\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-494a6aa0639b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#run and train until photos meet expectations (stop & restart model with tweaks if loss goes to 0 in discriminator)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m#shuffle L and AB channels then take a subset corresponding to each networks training size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_L\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_L\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#run and train until photos meet expectations (stop & restart model with tweaks if loss goes to 0 in discriminator)\n",
    "for epoch in tqdm(range(1,num_epochs+1)):\n",
    "    #shuffle L and AB channels then take a subset corresponding to each networks training size\n",
    "    np.random.shuffle(X_train_L)\n",
    "    l = X_train_L[:n]\n",
    "    np.random.shuffle(X_train_AB)\n",
    "    ab = X_train_AB[:160]\n",
    "    \n",
    "    fake_images = generator.predict(l[:160], verbose=1)\n",
    "    \n",
    "    #Train on Real AB channels\n",
    "    d_loss_real = discriminator.fit(x=ab, y= y_train_real,batch_size=32,epochs=1,verbose=1) \n",
    "    disc_real_losses.append(d_loss_real.history['loss'][-1])\n",
    "    \n",
    "    #Train on fake AB channels\n",
    "    d_loss_fake = discriminator.fit(x=fake_images,y=y_train_fake,batch_size=32,epochs=1,verbose=1)\n",
    "    disc_fake_losses.append(d_loss_fake.history['loss'][-1])\n",
    "    \n",
    "    #append the loss and accuracy and print loss\n",
    "    disc_acc.append(d_loss_fake.history['acc'][-1])\n",
    "    \n",
    "\n",
    "    #Train the gan by producing AB channels from L\n",
    "    g_loss = combined_network.fit(x=l, y=y_gen,batch_size=32,epochs=1,verbose=1)\n",
    "    #append and print generator loss\n",
    "    gen_losses.append(g_loss.history['loss'][-1])\n",
    "   \n",
    "    #every 50 epochs it prints a generated photo and every 100 it saves the model under that epoch\n",
    "    if epoch % 50 == 0:\n",
    "        print('Reached epoch:',epoch)\n",
    "        pred = generator.predict(X_test_L[2].reshape(1,256,256,1))\n",
    "        img = lab_to_rgb(np.dstack((X_test_L[2],pred.reshape(256,256,2))))\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        if epoch % 100 == 0:\n",
    "              generator.save('generator_' + str(epoch)+ '_v3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
